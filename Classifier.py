# -*- coding: utf-8 -*-
"""Classifier3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-tIr57QFO1H5gRGfWEHuq0UGHr4AOjji

Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import pickle
import glob
import os

from google.colab.patches import cv2_imshow
from google.colab import drive

from collections import Counter
import numpy as np
from PIL import Image
import itertools
import random
import copy
import time
import math

import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
import torch as T
import torch

from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, ChainDataset, IterableDataset
from torch.utils.data.sampler import Sampler, RandomSampler, SubsetRandomSampler, SequentialSampler, BatchSampler
from torchvision import transforms, utils, datasets

"""Mount Google Drive"""

# Commented out IPython magic to ensure Python compatibility.
drive.mount('/content/gdrive/')

print('\nSwitching working folder:')
# %cd '/content/gdrive/My Drive/Dissertation'

print('\nRoot contents:')
!ls

"""Verify CUDA"""

assert T.cuda.is_available(), 'No CUDA device detected.'

device_number = T.cuda.current_device()
print('Cuda avaibale:', T.cuda.is_available())
print('Cuda devices:', T.cuda.device_count())
print('Name:', T.cuda.get_device_name(device_number))

device = T.device('cuda:0' if torch.cuda.is_available() else 'cpu')

"""Class Declarations"""

class RedundancyCrop(object):
    """Squares off a 1280x720p PIL image by cropping the side of 
        the image that contains no information.
    Args:
        None.
    """

    def __init__(self):
      pass

    def __call__(self, img):
        """
        Args:
            img (PIL Image): Image to be cropped.
        Returns:
            PIL Image: Cropped image.
        """
        np_img = np.array(img)
        l_bbar = np_img[:,:560]
        l_img = np_img[:,560:]
        r_bbar = np_img[:,720:]
        r_img = np_img[:,:720]

        if np.any(l_bbar):
          return Image.fromarray(r_img)
        elif np.any(r_bbar):
          return Image.fromarray(l_img)

    def __repr__(self):
        return self.__class__.__name__

class DataWrapper(object):
  def __init__(self, data, folds=5, seed=2):
    self.data = data
    self.folds = folds
    self.current_fold = None
    self.samples = len(data)
    self.folded_data = self.split()
    self.classes = [class_names[2:] for class_names in self.data.classes]

    self.train_loader = None
    self.validate_loader = None
    self.test_loader = None

  def split(self, folds=5, seed=0):
    self.folds = folds

    split_length = int(np.floor(self.samples/self.folds))
    partial_split_length = self.samples % self.folds

    splitting = [split_length]*folds
    splitting.append(partial_split_length)

    folded_data = random_split(self.data, splitting, generator=torch.Generator().manual_seed(seed))
    del folded_data[-1] # Drop incomplete fold
    
    return folded_data

  def load(self, batch_size=4, seed=None):

    if seed is not None:
      self.current_fold = seed
    elif self.current_fold is None:
      self.current_fold = 0
    else:
      self.current_fold += 1

    idxs = np.roll(np.arange(self.folds), self.current_fold)

    ls = tuple([self.folded_data[idx] for idx in idxs[:-2]])

    train = ConcatDataset(tuple([self.folded_data[idx] for idx in idxs[:-2]]))
    validate =  self.folded_data[idxs[-2]]
    test = self.folded_data[idxs[-1]]

    self.train_loader = DataLoader(train, batch_size=batch_size, num_workers=0, pin_memory=True, drop_last=True)
    self.validate_loader = DataLoader(validate, batch_size = batch_size, num_workers=0, pin_memory=True, drop_last=True)
    self.test_loader = DataLoader(test, batch_size = batch_size, num_workers=0, pin_memory=True, drop_last=True)

  def show(self):

    for idx, fold in enumerate(self.folded_data):
      class_distributions = dict(Counter(elem[1] for elem in fold))
      class_0 = class_distributions.get(0)
      class_1 = class_distributions.get(1)

      print('\nFold %d: %d' % (idx, len(fold)))
      print('  Class 0: %d' % (0 if class_0 is None else class_0))
      print('  Class 1: %d' % (0 if class_1 is None else class_1))

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.conv1 = nn.Conv2d(1, 4, kernel_size=7, stride=2)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(4, 4, kernel_size=5, stride=2)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.conv3 = nn.Conv2d(4, 4, kernel_size=3, stride=1)
        self.conv4 = nn.Conv2d(4, 4, kernel_size=3, stride=1)
        self.conv5 = nn.Conv2d(4, 4, kernel_size=3, stride=1)
        self.pool5 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(4 * 18 * 18, 4096)
        self.fc2 = nn.Linear(4096, 2048)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = nn.LayerNorm(x.size()[1:])(x)
        x = self.pool1(x)
        x = self.pool2(F.relu(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.pool5(F.relu(self.conv5(x)))
        x = x.view(-1, 4 * 18 * 18)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x

class Classifier(object):
  def __init__(self, data):
    self.data = data
    self.network = Network()
    self.criterion = nn.CrossEntropyLoss()
    self.optimiser = optim.SGD(self.network.parameters(), lr=0.001, momentum=0.9)
    self.accuracy = [None]*self.data.folds
    
  def train(self, interval=100):
      running_loss = 0.0
      running_losses = []
      running_accuracy = []

      for idx, batch in enumerate(data.train_loader):
          inputs, labels = batch
      
          self.optimiser.zero_grad()   # Zero parameter gradients

          outputs = self.network(inputs)
          loss = self.criterion(outputs, labels)
          loss.backward()
          self.optimiser.step()

          running_loss += loss.item()
          if idx % interval == interval - 1:    # print every 100 mini-batches
              print('Minibatch: %3d | Loss -> %.3f' % (idx + 1, running_loss / interval))
              running_losses.append(running_loss / interval)
              running_loss = 0.0
              running_accuracy.append(self.validate())

      return running_losses, running_accuracy

  def validate(self, test=False):
    if test:
      data = self.data.test_loader
    else:
      data = self.data.validate_loader

    correct = 0
    total = 0
    with T.no_grad():
        for batch in data:
            samples, labels = batch
            outputs = self.network(samples)
            _, predicted = T.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    
    if test:
      self.accuracy[self.data.current_fold%self.data.folds] = accuracy
      print('Test Accuracy (Unseen): %d %%' % (accuracy))
    else:
      print('Validated @ %d %%' % (accuracy))

    return accuracy

  def nextFold(self):
    self.data.load()

  def resetNetwork(self):
    self.network = Network()
    self.criterion = nn.CrossEntropyLoss()
    self.optimiser = optim.SGD(self.network.parameters(), lr=0.001, momentum=0.9)

  def loadNetwork(self, fn):
    PATH = './' + fn + '.pth'
    self.network.load_state_dict(T.load(PATH))

  def saveModel(self, fn='default'):
    PATH = './' + fn + '.pth'
    T.save(self.network.state_dict(), PATH)

"""Data Import"""

transform = transforms.Compose([transforms.Grayscale(), RedundancyCrop(), transforms.ToTensor()])
raw_data = datasets.ImageFolder(root='./data', transform=transform)

data = DataWrapper(raw_data)
data.show()

"""Training & Testing"""

classifier = Classifier(data)
interval = 10
losses = []
accuracies = []
validations = []


for fold in range(classifier.data.folds):
  classifier.resetNetwork()
  classifier.nextFold()

  running_losses = []
  running_accuracies = []

  for epoch in range(2):
    print('\nFold: %d - Epoch %d '% (fold, epoch+1))

    running_loss, running_accuracy = classifier.train(interval)

    running_losses.append(running_loss)
    running_accuracies.append(running_accuracy)

  validations.append(classifier.validate(test=True))
  losses.append(running_losses)
  accuracies.append(running_accuracies)


print(classifier.accuracy)
print(sum(classifier.accuracy)/len(classifier.accuracy))

"""Loss and Accuracy Plots"""

sorted_losses = [[y for x in l for y in x] for l in losses]
sorted_accuracies = [[y for x in l for y in x] for l in accuracies]
batch_intervals = [x for x in range(interval,1+interval*len(sorted_losses[0]),interval)]

plt.rcParams.update({'font.size': 15})
plt.figure(figsize=(27,7))
plt.subplot(121)
plt.xlabel('Minibatches')
plt.ylabel('Loss')
for i, sl in enumerate(sorted_losses):
  plt.plot(batch_intervals, sl, label='Fold ' + str(i))
plt.legend()

plt.subplot(122)
plt.xlabel('Minibatches')
plt.ylabel('Accuracy')
for i, sa in enumerate(sorted_accuracies):
  plt.plot(batch_intervals, sa, label='Fold ' + str(i))
plt.legend()
plt.suptitle('Training Plots')
plt.show()

"""Unseen Data Plot"""

unseen_accuracy = classifier.accuracy
unseen_average = sum(unseen_accuracy)/5

labels = ['Fold 0', 'Fold 1', 'Fold 2', 'Fold 3', 'Fold 4']
label_pos = np.arange(len(labels))

plt.rcParams.update({'font.size': 15})
plt.figure(figsize=(10,6))
plt.subplot(111)
plt.grid(axis='y')
plt.ylim(90, 102)
plt.bar(label_pos, unseen_accuracy, align='center', alpha=0.8)
plt.xticks(label_pos, labels)
plt.axhline(unseen_average, color='r', linestyle='--')

plt.suptitle('Performance On Unseen Data')
plt.show()
